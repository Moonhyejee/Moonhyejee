import numpy as np     #numpy를 np로 호출
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf     #tenseorflow를 tf로 호출
import tensorflow.contrib.eager as tfe  #tensorflow에서 eager모드로 실행하기 위한 기본적인 libarary입력

tf.enable_eager_execution()   #tensorflow를 명령형 프로그래밍 환경으로 바꿔줌(즉시 실행)
tf.set_random_seed(777)  # for reproducibility  #랜덤으로 돌리기
print(tf.__version__)


x_train = [[1., 2.],   #x_data 입력
          [2., 3.],
          [3., 1.],
          [4., 3.],
          [5., 3.],
          [6., 2.]]
y_train = [[0.],     #y_data 입력
          [0.],
          [0.],
          [1.],
          [1.],
          [1.]]

x_test = [[5.,2.]]   #test_data
y_test = [[1.]]      #test_data

dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()   #tf.data를 통해서 X값과 Y값을 X의 길이만큼 batch로 학습을 하겠다는 것을 뜻함. 

W = tf.Variable(tf.zeros([2,1]), name='weight')   #2행 1열로 만듦. 
b = tf.Variable(tf.zeros([1]), name='bias')       #bias값을 만듦.

def logistic_regression(features):
    hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))   #hypothesis(시그모이드 함수)를 그려냄
    return hypothesis
    
def loss_fn(hypothesis, features, labels):    #hypothesis로 나온 값을 lables로 넣음
    cost = -tf.reduce_mean(labels * tf.log(logistic_regression(features)) + (1 - labels) * tf.log(1 - hypothesis))  #hypothesis와 lables값을 이용하여 cost값을 구함
    return cost                           #logistic_regression(features)은 features이 들어오기 때문에 코드상으로 오류를 얻기 위해 호출

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)   #GradientDescentOptimizer를 통해 실제 learning_rate를 통한 값으로 optimizer를 선언

def accuracy_fn(hypothesis, labels):    #가설과 실제를 비교하기 위한 함수 정의 
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)  #hypothesis값이 0.5를 통해 예측된 값이 나오므로 hypothesis > 0.5라고 씀
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))   #predicted값이 실제로 맞는지 안맞는지 출력
    return accuracy
    
def grad(hypothesis, features, labels):   #학습을 위한 hypothesis, labels값이 나오면
    with tf.GradientTape() as tape:   #gradient descent 알고리즘을 GrandientTape을 가지고 구현. 변수들에 대한 정보를 tape로 입력
        loss_value = loss_fn(logistic_regression(features),features,labels)  #loss값에다가 가설을 통해나온 값과 실제값을 비교한 값을 구할 수 있음.
    return tape.gradient(loss_value, [W,b])    #gradient를 통해서 실제 모델값을 계속 바꿔갈 수 있음
    
EPOCHS = 1001   

for step in range(EPOCHS):   #EPOCHS(1001)번 반복
    for features, labels  in tfe.Iterator(dataset):   #data값을 가져와서 입력
        grads = grad(logistic_regression(features), features, labels)  #나온 값을 가설에 집어 넣어서 gradient값을 호출
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))     #optimizer을 통해서 minimize를 구함(최적의 값)
        if step % 100 == 0:   #중간중간 확인
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))  #100번마다 출력
test_acc = accuracy_fn(logistic_regression(x_test),y_test)   #test_data 값을 출력
print("Testset Accuracy: {:.4f}".format(test_acc))


#결과
Iter: 0, Loss: 0.6874
Iter: 100, Loss: 0.5776
Iter: 200, Loss: 0.5349
Iter: 300, Loss: 0.5054
Iter: 400, Loss: 0.4838
Iter: 500, Loss: 0.4671
Iter: 600, Loss: 0.4535
Iter: 700, Loss: 0.4420
Iter: 800, Loss: 0.4319
Iter: 900, Loss: 0.4228
Iter: 1000, Loss: 0.4144
Testset Accuracy: 1.0000
